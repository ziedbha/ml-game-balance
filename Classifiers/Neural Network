{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Neural Network","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"_sVL1HaV4fdr","colab_type":"text"},"cell_type":"markdown","source":["# ReadMe\n","\n","**How to Setup:**\n","\n","\n","*   Download the Casual and Professional datasets and change the \"professional_url\" variable to the path of the professional dataset, and change the \"ranked_url\" variable to the path of the casual dataset.\n","*   For both professional and casual there are two different types of data, \"Vanilla\" and \"Winrates\". \"Vanilla\" for casual only have as features a one hot encoding of the champions played and the individual player skill levels. In the professional dataset it contains a one hot encoding of the champions played and a one hot encoding of the players on each team. For \"Winrates\" it contains the previously mentioned features, but in addition the compiled winrates for champion-champion matchups and individual champion winrates.\n","*   To switch between \"Winrates\" and \"Vanilla\", change the data_type to the appropriate string. To switch between the professional and casual dataset change casual to False or True respectively.\n","*   Set the batch size and number of epochs to run\n","\n","**How to Run:**\n","*   To run, simply set up as specified above and then run all code cells consecutively.\n","\n","**Error Handling:**\n","*    In the case of an error, reset the runtime and run the code cells consecutively again.\n","\n","\n"]},{"metadata":{"id":"ETZZI0u44afc","colab_type":"text"},"cell_type":"markdown","source":["# Importing Packages"]},{"metadata":{"id":"v2EVrN4JzVBj","colab_type":"code","colab":{}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"U_JypCC3z67X","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.image as mpimg\n","from torch.utils.data.dataset import Dataset\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.data import DataLoader\n","import matplotlib.patches as patches\n","import os\n","import math\n","from os import listdir\n","from os.path import isfile, join\n","from PIL import Image\n","from torch.autograd import Variable\n","import cv2\n","import torchvision.models as models\n","import random \n","import pandas as pd\n","import numpy as np\n","from sklearn import preprocessing\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","import seaborn as sns\n","import matplotlib.pyplot as plt \n","import pdb\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.decomposition import PCA\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3XzvYtlH69ek","colab_type":"text"},"cell_type":"markdown","source":["# Program Parameter Setup\n","\n","Please change the values in this cell for your specific use."]},{"metadata":{"id":"Eh01uChUhbq1","colab_type":"code","colab":{}},"cell_type":"code","source":["# Different data types are Vanilla and Winrates\n","data_type = \"Winrates\"\n","casual = True\n","professional_url = \"/content/gdrive/Team Drives/CIS 520/professional/\"\n","ranked_url = \"/content/gdrive/Team Drives/CIS 520/casual/\"\n","batch_size = 50\n","num_epochs = 1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OtrKcabrNy2e","colab_type":"text"},"cell_type":"markdown","source":["# **Data Importing**"]},{"metadata":{"id":"MPmCmvH1V8HX","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","#vanilla\n","X_pro_filename_train = professional_url + \"train/Professional_Features_\" + data_type + \"_Train.csv\"\n","\n","y_pro_filename_train = professional_url + \"train/Professional_Labels_Train.csv\"\n","\n","X_pro_train = pd.read_csv(X_pro_filename_train)\n","y_pro_train = pd.read_csv(y_pro_filename_train)\n","\n","#vanilla\n","X_pro_filename_test = professional_url + \"test/Professional_Features_\" + data_type + \"_Test.csv\"\n","\n","y_pro_filename_test = professional_url + \"test/Professional_Labels_Test.csv\"\n","\n","X_pro_test = pd.read_csv(X_pro_filename_test)\n","y_pro_test = pd.read_csv(y_pro_filename_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cv-OfPaNyZJ9","colab_type":"code","colab":{}},"cell_type":"code","source":["#vanilla\n","X_ranked_filename_train = ranked_url + \"train/Casual_Features_\" + data_type + \"_Train.csv\"\n","\n","y_ranked_filename_train = ranked_url + \"train/Casual_Labels_Train.csv\"\n","\n","X_casual_train = pd.read_csv(X_ranked_filename_train)\n","y_casual_train = pd.read_csv(y_ranked_filename_train)\n","\n","#vanilla\n","X_ranked_filename_test = ranked_url + \"test/Casual_Features_\" + data_type + \"_Test.csv\"\n","\n","y_ranked_filename_test = ranked_url + \"test/Casual_Labels_Test.csv\"\n","\n","X_casual_test = pd.read_csv(X_ranked_filename_test)\n","y_casual_test = pd.read_csv(y_ranked_filename_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uQocRppNNq8s","colab_type":"text"},"cell_type":"markdown","source":["# **Loading Data**"]},{"metadata":{"id":"i63UftpEoBGf","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train = None\n","y_train = None\n","X_test = None\n","Y_test = None\n","if (not casual):\n","  X_train = X_pro_train\n","  y_train = y_pro_train\n","  X_test = X_pro_test\n","  y_test = y_pro_test\n","else:\n","  X_train = X_casual_train\n","  y_train = y_casual_train\n","  X_test = X_casual_test\n","  y_test = y_casual_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HhBN3Cdet3o8","colab_type":"code","colab":{}},"cell_type":"code","source":["# convert to categorical variables\n","\n","cat_vars = list(X_train)\n","\n","for var in cat_vars:\n","  if (\"winrate\" in var) or (\"player\" in var) or (\"skill\" in var):\n","    continue\n","  else:\n","    X_train[var] = X_train[var].astype('category')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"mCY-MQE6t7qi","colab_type":"code","colab":{}},"cell_type":"code","source":["# one-hot encoding\n","\n","cat_vars = list(X_train)\n","\n","split_at = X_train.shape[0]\n","full_data = X_train.append(X_test)\n","\n","for var in cat_vars:\n","  if (\"winrate\" in var) or (\"player\" in var) or (\"skill\" in var):\n","    continue\n","  else:\n","    cat_list = pd.get_dummies(full_data[var], prefix = var) # makes every variable binary\n","    full_data = pd.concat([full_data, cat_list], axis=1)\n","\n","full_data.drop(columns = ['blue_top', 'red_top', 'blue_jungle', 'red_jungle', 'blue_middle','red_middle', 'blue_adc', 'red_adc', 'blue_support', 'red_support'], inplace=True)\n","  \n","X_train_encoded = full_data[0:split_at]\n","X_test_encoded = full_data[split_at:full_data.shape[0]]\n","\n","X_train = X_train_encoded\n","X_test = X_test_encoded"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E8PEGcCVtHDt","colab_type":"text"},"cell_type":"markdown","source":["## Dataloader"]},{"metadata":{"id":"-Fu1LslBtN_a","colab_type":"code","colab":{}},"cell_type":"code","source":["class Dataset(Dataset):\n","    \"\"\"Face Landmarks dataset.\"\"\"\n","\n","    def __init__(self, train, labels):\n","        self.dataset = train\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        feature_vec = self.dataset.iloc[idx].as_matrix().astype(int)\n","        label = self.labels.iloc[\n","            idx]\n","        sample = {'data': feature_vec, 'label': np.array([label])}\n","\n","        return sample"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z9wH8zpuwc7a","colab_type":"code","colab":{}},"cell_type":"code","source":["train_dataset = Dataset(X_train, y_train)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_dataset = Dataset(X_test, y_test)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uGDk6VPws7DW","colab_type":"text"},"cell_type":"markdown","source":["## Creating Neural Network Model"]},{"metadata":{"id":"ERunfdRrsq67","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","if not casual:\n","  if data_type == \"Vanilla\":\n","    class Net(nn.Module):\n","      def __init__(self):\n","          super(Net, self).__init__()\n","          self.fc1 = nn.Linear(532, 200)\n","          self.fc1_bn = nn.BatchNorm1d(200)\n","          self.fc2 = nn.Linear(200, 100)\n","          self.fc2_bn = nn.BatchNorm1d(100)\n","          self.fc3 = nn.Linear(100, 25)\n","          self.fc3_bn = nn.BatchNorm1d(25)\n","          self.fc4 = nn.Linear(25, 1)\n","\n","\n","      def forward(self, x):\n","          if x.shape[0] > 1:\n","            x = F.relu(self.fc1_bn(self.fc1(x)))\n","            x = F.relu(self.fc2_bn(self.fc2(x)))\n","            x = F.relu(self.fc3_bn(self.fc3(x)))\n","            x = self.fc4(x)\n","          else:\n","            x = F.relu(self.fc1(x))\n","            x = F.relu(self.fc2(x))\n","            x = F.relu(self.fc3(x))\n","            x = self.fc4(x)\n","          return torch.sigmoid(x)\n","  else:\n","    class Net(nn.Module):\n","      def __init__(self):\n","          super(Net, self).__init__()\n","          self.fc1 = nn.Linear(547, 200)\n","          self.fc1_bn = nn.BatchNorm1d(200)\n","          self.fc2 = nn.Linear(200, 100)\n","          self.fc2_bn = nn.BatchNorm1d(100)\n","          self.fc3 = nn.Linear(100, 25)\n","          self.fc3_bn = nn.BatchNorm1d(25)\n","          self.fc4 = nn.Linear(25, 1)\n","\n","\n","      def forward(self, x):\n","          if x.shape[0] > 1:\n","            x = F.relu(self.fc1_bn(self.fc1(x)))\n","            x = F.relu(self.fc2_bn(self.fc2(x)))\n","            x = F.relu(self.fc3_bn(self.fc3(x)))\n","            x = self.fc4(x)\n","          else:\n","            x = F.relu(self.fc1(x))\n","            x = F.relu(self.fc2(x))\n","            x = F.relu(self.fc3(x))\n","            x = self.fc4(x)\n","          return torch.sigmoid(x)\n","else:\n","  if data_type == \"Vanilla\":\n","    class Net(nn.Module):\n","      def __init__(self):\n","          super(Net, self).__init__()\n","          self.fc1 = nn.Linear(831, 400)\n","          self.fc1_bn = nn.BatchNorm1d(400)\n","          self.fc2 = nn.Linear(400, 200)\n","          self.fc2_bn = nn.BatchNorm1d(200)\n","          self.fc3 = nn.Linear(200, 100)\n","          self.fc3_bn = nn.BatchNorm1d(100)\n","          self.fc4 = nn.Linear(100, 1)\n","\n","\n","      def forward(self, x):\n","        if x.shape[0] > 1:\n","          x = F.relu(self.fc1_bn(self.fc1(x)))\n","          x = F.relu(self.fc2_bn(self.fc2(x)))\n","          x = F.relu(self.fc3_bn(self.fc3(x)))\n","          x = self.fc4(x)\n","        else:\n","          x = F.relu(self.fc1(x))\n","          x = F.relu(self.fc2(x))\n","          x = F.relu(self.fc3(x))\n","          x = self.fc4(x)\n","        return torch.sigmoid(x)\n","  else:\n","    class Net(nn.Module):\n","        def __init__(self):\n","            super(Net, self).__init__()\n","            self.fc1 = nn.Linear(846, 400)\n","            self.fc1_bn = nn.BatchNorm1d(400)\n","            self.fc2 = nn.Linear(400, 200)\n","            self.fc2_bn = nn.BatchNorm1d(200)\n","            self.fc3 = nn.Linear(200, 100)\n","            self.fc3_bn = nn.BatchNorm1d(100)\n","            self.fc4 = nn.Linear(100, 1)\n","\n","\n","        def forward(self, x):\n","          if x.shape[0] > 1:\n","            x = F.relu(self.fc1_bn(self.fc1(x)))\n","            x = F.relu(self.fc2_bn(self.fc2(x)))\n","            x = F.relu(self.fc3_bn(self.fc3(x)))\n","            x = self.fc4(x)\n","          else:\n","            x = F.relu(self.fc1(x))\n","            x = F.relu(self.fc2(x))\n","            x = F.relu(self.fc3(x))\n","            x = self.fc4(x)\n","          return torch.sigmoid(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"q6t5BL2RN2we","colab_type":"text"},"cell_type":"markdown","source":["# **Training / Testing**"]},{"metadata":{"id":"5PJ7KhhU6Sjk","colab_type":"text"},"cell_type":"markdown","source":["## Training Loop"]},{"metadata":{"id":"bEAtMrDZ6Rpa","colab_type":"code","colab":{}},"cell_type":"code","source":["net = Net()\n","net.to(device)\n","print(net)\n","\n","\n","criterion = nn.BCELoss()\n","# optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n","optimizer = optim.Adam(net.parameters())\n","accuracy_array = []\n","loss_array = []\n","# pdb.set_trace()\n","\n","\n","for epoch in range(num_epochs): \n","\n","    running_loss = 0.0\n","    accuracy = 0\n","    running_accuracy = 0\n","    curr_iters = 0\n","    for i, data in enumerate(train_loader, 0):\n","        # get the inputs\n","        inputs = data[\"data\"]\n","        labels = data[\"label\"]\n","        inputs, labels = inputs.type(torch.FloatTensor).to(device), labels.to(device).float()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        \n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        if (outputs.shape[0] > 1):\n","          outputs = outputs.squeeze()\n","          labels = labels.squeeze()\n","        else:\n","          outputs = outputs.reshape([1])\n","          labels = labels.reshape([1])\n","        \n","        output_labels = np.round(outputs.cpu().detach().numpy(), 0)\n","        \n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        \n","        running_accuracy += np.sum(np.equal(output_labels, labels.cpu().detach().numpy()))\n","        curr_iters += len(labels)\n","        running_loss += loss.item()\n","        \n","        optimizer.step()\n","        \n","        \n","        if (curr_iters > 1000):\n","          loss_array.append(running_loss / curr_iters)\n","          accuracy_array.append(running_accuracy / curr_iters)\n","          print('[%d, %5d] loss: %.3f accuracy: %.3f' %(epoch + 1, i + 1, running_loss / curr_iters, running_accuracy / curr_iters))\n","          running_loss = 0.0\n","          running_accuracy = 0.0\n","          curr_iters = 0\n","    \n","    running_loss = 0.0\n","    \n","print('Finished Training')\n","plt.figure()\n","plt.title(\"Accuracy vs Iterations\")\n","plt.xlabel(\"Iterations (1000s)\")\n","plt.ylabel(\"Accuracy\")\n","plt.plot(accuracy_array)\n","plt.figure()\n","plt.title(\"Loss vs Iterations\")\n","plt.xlabel(\"Iterations (1000s)\")\n","plt.ylabel(\"Loss\")\n","plt.plot(loss_array)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dYyZSw9T6jO7","colab_type":"text"},"cell_type":"markdown","source":["## Testing Loop"]},{"metadata":{"id":"1YHmkmCKCWmG","colab_type":"code","colab":{}},"cell_type":"code","source":["correct = 0\n","false_negatives = 0\n","false_positives = 0\n","true_negatives = 0\n","true_positives = 0\n","total = 0\n","with torch.no_grad():\n","    for data in test_loader:\n","        inputs = data[\"data\"]\n","        labels = data[\"label\"]\n","        inputs, labels = inputs.type(torch.FloatTensor).to(device), labels.to(device).float()\n","\n","        optimizer.zero_grad()\n","        \n","        outputs = net(inputs)\n","        if (outputs.shape[0] > 1):\n","          outputs = outputs.squeeze()\n","          labels = labels.squeeze()\n","        else:\n","          outputs = outputs.reshape([1])\n","          labels = labels.reshape([1])\n","        \n","        output_labels = np.round(outputs.cpu().detach().numpy(), 0)\n","        true_positives += np.sum((output_labels == 1) & (labels.cpu().detach().numpy() == 1))\n","        true_negatives += np.sum((output_labels == 0) & (labels.cpu().detach().numpy() == 0))\n","        false_negatives += np.sum((output_labels == 0) & (labels.cpu().detach().numpy() == 1))\n","        false_positives += np.sum((output_labels == 1) & (labels.cpu().detach().numpy() == 0))\n","        \n","        accuracy = np.sum(np.equal(output_labels, labels.cpu().detach().numpy()))/len(labels)\n","        \n","        running_loss += loss.item()\n","        accuracy_array.append(accuracy)\n","        \n","        total += len(labels)\n","        correct += np.sum(np.equal(output_labels, labels.cpu().detach().numpy()))\n","\n","print('Accuracy of the network on the test images: %d %%' % (\n","    100 * correct / total))\n","print('True Positive Rate: %f %%, False Positive Rate: %f %%, True Negative Rate: %f %%, False Negative Rate: %f %%' % (\n","    float(true_positives) / total, float(false_positives) / total, float(true_negatives) / total, float(false_negatives) / total))\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2w6OzjvSGOOd","colab_type":"text"},"cell_type":"markdown","source":["# Plotting Utilities"]},{"metadata":{"id":"s_RouMidWfsw","colab_type":"code","colab":{}},"cell_type":"code","source":["plt.figure()\n","plt.title(\"Accuracy vs Batch Size\")\n","plt.xlabel(\"Batch Size\")\n","plt.ylabel(\"Accuracy\")\n","plt.plot([1 , 5, 10, 15, 20, 50, 100], [.55, .55, .54, .53, .54, .56, .53])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1kXzcx01C0VR","colab_type":"code","colab":{}},"cell_type":"code","source":["components = 1\n","variance_threshold = 0.9\n","delta = 5\n","variances = []\n","x_array = []\n","\n","while True:\n","  curr_iter = components - 1\n","  pca = PCA(n_components=components)\n","  pca.fit(X_train_encoded)\n","  explained_variance = np.sum(pca.explained_variance_ratio_);\n","  if curr_iter % delta == 0:\n","    variances.append(explained_variance)\n","    x_array.append(components)\n","  if explained_variance > variance_threshold:\n","    break\n","  else:\n","    components +=1\n","plt.bar(x_array, variances, width = 3)\n","plt.title(\"Explained Variance vs. # of Principal Components\")\n","plt.xlabel(\"# of Principal Components\")\n","plt.ylabel(\"Explained Variance\")"],"execution_count":0,"outputs":[]}]}